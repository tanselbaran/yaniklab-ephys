{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFP Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs through the process of stimulus evoked LFP analysis step by step to the end figures. Please follow the upcoming steps to successfully analyze your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Copy your data from HDD to SSD \n",
    "\n",
    "First things first! Before any data analysis, we need to <b>copy the data from the hard drive where the raw data is stored to the 1 TB SSD on this computer that is designated as the workspace</b>. This is asked for two reasons: \n",
    "\n",
    "<ol type=\"1\">\n",
    "<li>There is a remarkable difference between the speed of reading data from the HDD vs. SSD</li>\n",
    "<li>There are some by-products of the data analysis that should be deleted at the end of the analysis. If these files are stored in the HDD, Dropbox tries to synchonize all those files as well. And also some times these files are forgotten to be deleted and end up taking precious space on the HDD where more novel, raw data could have been stored.</li>\n",
    "</ol>\n",
    "\n",
    "<b> IF </b> you have completed this step, please continue with the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Generate parameter dictionary for all recording sessions\n",
    "\n",
    "Next, we will need to generate pickle files named <i> paramsDict.p </i> for each recording session in your experiment. These pickle files contain crucial parameters related to data acquisition and your preferences on the details of how the data should be analyzed. For this procedure, please <b> run the block below </b> to launch the Jupyter notebook (Python 3) for generating the parameter dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaEnvironmentNotFoundError: Could not find environment: klusta .\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\u001b[32m[I 13:46:56.479 NotebookApp]\u001b[0;10m The port 8888 is already in use, trying another port.\n",
      "\u001b[32m[I 13:46:56.507 NotebookApp]\u001b[0;10m JupyterLab alpha preview extension loaded from /Users/aagamshah/anaconda3/lib/python3.6/site-packages/jupyterlab\n",
      "JupyterLab v0.27.0\n",
      "Known labextensions:\n",
      "\u001b[32m[I 13:46:56.509 NotebookApp]\u001b[0;10m Running the core application with no additional extensions or settings\n",
      "\u001b[32m[I 13:46:56.514 NotebookApp]\u001b[0;10m Serving notebooks from local directory: /Users/aagamshah/Documents/GitHub/yaniklab_ephys\n",
      "\u001b[32m[I 13:46:56.514 NotebookApp]\u001b[0;10m 0 active kernels \n",
      "\u001b[32m[I 13:46:56.515 NotebookApp]\u001b[0;10m The Jupyter Notebook is running at: http://localhost:8889/?token=0b559c07c9ad8f17d581ab6d7e5323e7939bf5ea9dd09cb6\n",
      "\u001b[32m[I 13:46:56.515 NotebookApp]\u001b[0;10m Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
      "[C 13:46:56.519 NotebookApp] \n",
      "    \n",
      "    Copy/paste this URL into your browser when you connect for the first time,\n",
      "    to login with a token:\n",
      "        http://localhost:8889/?token=0b559c07c9ad8f17d581ab6d7e5323e7939bf5ea9dd09cb6\n",
      "\u001b[32m[I 13:46:56.923 NotebookApp]\u001b[0;10m Accepting one-time-token-authenticated connection from ::1\n",
      "\u001b[33m[W 13:46:57.635 NotebookApp]\u001b[0;10m Notebook Generate_dict_for_experiment.ipynb is not trusted\n",
      "\u001b[32m[I 13:46:58.096 NotebookApp]\u001b[0;10m Kernel started: 160ca4d1-557c-4d4f-825e-14c89ab2d335\n",
      "\u001b[32m[I 13:46:59.155 NotebookApp]\u001b[0;10m Adapting to protocol v5.1 for kernel 160ca4d1-557c-4d4f-825e-14c89ab2d335\n"
     ]
    }
   ],
   "source": [
    "source activate klusta #Activating the klustakwik virtual environment\n",
    "jupyter notebook './Generate_dict_for_experiment.ipynb'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.5) Restart kernel\n",
    "\n",
    "Since no bash or python command to this date exists for properly closing a jupyter notebook, from the <i> Kernel </i> tab above, select <i> Restart & Clear Output </i> to restart the kernel. That will not be an issue since the <i> paramsDict.p </i> files are already generated and saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Run the main analysis function on the data \n",
    "\n",
    "We are ready to perform the analysis on the data. Please first <b> specify the path (with / at the end) </b> to the folder that contains the folders for all recprding sessions (i.e. the folder right above the folders of recording sessions in hierarchy) and then <b> run the following line of code </b> to run the script <i> analyze_all_recording_sessions.py </i> which will run the <i> main </i> function in the <i> main_tetrode.py </i>. You can check the scripts for the details of the steps running on the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Specify the path here!\n",
    "PATHEXP=\"/path/to/the/folder/containing/the/folders/for/recording/sessions/\" \n",
    "echo $PATHEXP|python analyze_all_recording_sessions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Generate the figures and extract results\n",
    "\n",
    "As we have extracted the evoked LFP waveforms (and optionally ran the klusta, in case you enabled the spike sorting for this analysis), now we are ready to generate figures and  extract results for this experiment. These steps will be performed in the <i> evoked_lfp_analysis.py </i> script. Please <b> run the following line </b> to perform this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "echo $PATHEXP | python evoked_lfp_analysis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5) Window LFP analysis (optional)\n",
    " \n",
    " If there is any recording session that you would like to analyze as broken down into time windows, please run the following block to open the ipython notebook for analyzing evoked LFP data in time windows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jupyter notebook './Window_LFP_analysis.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done! \n",
    "\n",
    "You can check the <i> Analyzed </i> folder to see the plots and the excel sheets containing the evoked LFP data and <b> move the files and folders </b> that you deem necessary into the <i> Analyzed </i> folder inside the <i> Electrophysiology </i> Dropbox folder. Please do not forget to rename the folder with the date and the name of the experiment when adding to the <i> Analyzed </i> folder. At the end, please <b> delete the data and the intermediate files from the SSD. </b> \n",
    "\n",
    "Notebook written by Baran Yasar in 04/2017. Please contact him in person or via e-mail at yasar@biomed.ee.ethz.ch in case of any questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
